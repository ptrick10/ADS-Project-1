{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ccf43eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTaxi Data Model Implementation\\nWritten by Patrick Butcher\\nLast edited 16/08/2021\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Taxi Data Model Implementation\n",
    "Written by Patrick Butcher\n",
    "Last edited 16/08/2021\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7f6198a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/16 09:42:57 WARN Utils: Your hostname, DESKTOP-M8PQ28M resolves to a loopback address: 127.0.1.1; using 192.168.86.205 instead (on interface wifi0)\n",
      "21/08/16 09:42:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "21/08/16 09:42:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "INFO:SparkMonitorKernel:Client Connected ('127.0.0.1', 62126)\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Start the spark context\n",
    "sc = SparkContext.getOrCreate(conf=swan_spark_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372b2748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "\n",
    "# a nice way of filtering out deprecated warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# start a spark session (from spark tutrial)\n",
    "from pyspark.sql import SparkSession\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# create a spark session (which will run spark jobs)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d826758",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "454f8ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# schema\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "'''\n",
    "From Spark Tutorial, \n",
    "Author: Akira Takihara Wang,\n",
    "Edited by: Patrick Butcher\n",
    "'''\n",
    "\n",
    "feb_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-02.csv', header=True)\n",
    "\n",
    "ints = ('VendorID', 'passenger_count', 'RatecodeID', 'PULocationID', 'DOLocationID', 'payment_type',)\n",
    "doubles = ('trip_distance', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', \n",
    "           'improvement_surcharge', 'total_amount', 'congestion_surcharge')\n",
    "strings = ('store_and_fwd_flag', )\n",
    "dtimes = ('tpep_pickup_datetime', 'tpep_dropoff_datetime', )\n",
    "\n",
    "dtypes = {column: IntegerType() for column in ints}\n",
    "dtypes.update({column: DoubleType() for column in doubles})\n",
    "dtypes.update({column: StringType() for column in strings})\n",
    "dtypes.update({column: TimestampType() for column in dtimes})\n",
    "schema = StructType()\n",
    "\n",
    "for column in feb_yellow_sdf.columns:\n",
    "    schema.add(column, # column name\n",
    "               dtypes[column], # data type\n",
    "               True # is nullable?\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9b79696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data staight into spark dataframe\n",
    "jan_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-01.csv', header=True, schema=schema)\n",
    "feb_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-02.csv', header=True, schema=schema)\n",
    "mar_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-03.csv', header=True, schema=schema)\n",
    "apr_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-04.csv', header=True, schema=schema)\n",
    "may_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-05.csv', header=True, schema=schema)\n",
    "jun_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-06.csv', header=True, schema=schema)\n",
    "jul_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-07.csv', header=True, schema=schema)\n",
    "aug_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-08.csv', header=True, schema=schema)\n",
    "sep_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-09.csv', header=True, schema=schema)\n",
    "oct_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-10.csv', header=True, schema=schema)\n",
    "nov_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-11.csv', header=True, schema=schema)\n",
    "dec_yellow_sdf = spark.read.csv('../Proj1data/yellow_tripdata_2019-12.csv', header=True, schema=schema)\n",
    "\n",
    "jan_yellow_sdf_2020 = spark.read.csv('../Proj1data/yellow_tripdata_2020-01.csv', header=True, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f676c3bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# combine dataframes\n",
    "yellow_data_2019 = jan_yellow_sdf.union(feb_yellow_sdf).union(mar_yellow_sdf).union(apr_yellow_sdf).union(may_yellow_sdf).union(jun_yellow_sdf).union(jul_yellow_sdf).union(aug_yellow_sdf).union(sep_yellow_sdf).union(oct_yellow_sdf).union(nov_yellow_sdf).union(dec_yellow_sdf)\n",
    "yellow_data_2019.printSchema()\n",
    "\n",
    "del jan_yellow_sdf\n",
    "del feb_yellow_sdf\n",
    "del mar_yellow_sdf\n",
    "del apr_yellow_sdf\n",
    "del may_yellow_sdf\n",
    "del jun_yellow_sdf\n",
    "del jul_yellow_sdf\n",
    "del aug_yellow_sdf\n",
    "del sep_yellow_sdf\n",
    "del oct_yellow_sdf\n",
    "del nov_yellow_sdf\n",
    "del dec_yellow_sdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9467c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(date,DateType,true),StructField(maxTemp,StringType,true)))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Author: Andy Upton\n",
    "Accessed: 16/08/2021\n",
    "Edited by Patrick Butcher\n",
    "https://www.andyupton.net/blog/2019/6/12/feature-engineering-with-pyspark\n",
    "'''\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import to_date, dayofweek\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019.withColumn('trip_time', unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp('tpep_pickup_datetime'))\n",
    "\n",
    "# get max temperature for the day\n",
    "max_temperatures_sdf = spark.read.csv('../Proj1data/maxTemperaturesNYC.csv', header=True)\n",
    "# max_temperatures_sdf.select(to_date(max_temperatures_sdf.date).alias('date'))\n",
    "\n",
    "max_temperatures_sdf = max_temperatures_sdf.withColumn('date', to_date(max_temperatures_sdf['date']))\n",
    "\n",
    "print(max_temperatures_sdf.schema)\n",
    "\n",
    "# add column to yellow data\n",
    "# yellow_data_2019_w_temp = yellow_data_2019.withColumn('max_temp', when(yellow_data_2019.date, dic[date]))\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.withColumn('date', to_date(yellow_data_2019_wtemp_and_time.tpep_pickup_datetime))\n",
    "\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.join(max_temperatures_sdf, on=['date'], how='left_outer')\n",
    "\n",
    "## Same for 2020 \n",
    "max_temperatures_sdf_2020 = spark.read.csv('../Proj1data/maxTemperaturesNYC2020.csv', header=True)\n",
    "max_temperatures_sdf_2020 = max_temperatures_sdf_2020.withColumn('date', to_date(max_temperatures_sdf_2020['date']))\n",
    "\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020.withColumn('trip_time', unix_timestamp(\"tpep_dropoff_datetime\") - unix_timestamp('tpep_pickup_datetime'))\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.withColumn('date', to_date(jan_yellow_sdf_2020_wtemp_and_time.tpep_pickup_datetime))\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.join(max_temperatures_sdf_2020, on=['date'], how='left_outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91ef3e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change trip time to int for efficiency\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.withColumn('trip_time', yellow_data_2019_wtemp_and_time.trip_time.cast('int'))\n",
    "yellow_data_2019_wtemp_and_time.schema\n",
    "\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.withColumn('trip_time', jan_yellow_sdf_2020_wtemp_and_time.trip_time.cast('int'))\n",
    "\n",
    "# change temperature to double\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.withColumn('maxTemp', yellow_data_2019_wtemp_and_time.trip_time.cast('double'))\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.withColumn('maxTemp', jan_yellow_sdf_2020_wtemp_and_time.trip_time.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "125d2823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter data using filter\n",
    "\n",
    "# filter fare amount to be between 0 and 600\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.fare_amount > 0) \n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.fare_amount < 200)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.fare_amount > 0) \n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.fare_amount < 200)\n",
    "\n",
    "# filter trip distance to be between 0 and 150\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.trip_distance > 0)\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.trip_distance < 150)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.trip_distance > 0)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.trip_distance < 150)\n",
    "\n",
    "# filter payment type to be cash or card only\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.payment_type != 3)\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.payment_type != 4)\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.payment_type != 5)\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.payment_type != 6)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.payment_type != 3)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.payment_type != 4)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.payment_type != 5)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.payment_type != 6)\n",
    "\n",
    "# filter so rate code must be standard rate\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.RatecodeID == 1)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.RatecodeID == 1)\n",
    "\n",
    "# If temperature is missing, remove instance\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.maxTemp.isNotNull())\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.maxTemp.isNotNull())\n",
    "\n",
    "# # no trips an be > 2 hours\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.trip_time > 0)\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.filter(yellow_data_2019_wtemp_and_time.trip_time < 7200)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.trip_time > 0)\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.filter(jan_yellow_sdf_2020_wtemp_and_time.trip_time < 7200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7243bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change temp to double type\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.withColumn('maxTemp', yellow_data_2019_wtemp_and_time.trip_time.cast('double'))\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.withColumn('maxTemp', jan_yellow_sdf_2020_wtemp_and_time.trip_time.cast('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a877729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column of fare amount/trip time, so average dollar per second\n",
    "yellow_data_2019_wtemp_and_time = yellow_data_2019_wtemp_and_time.withColumn('fare_amount_per_sec', (col(\"fare_amount\") / col(\"trip_time\")))\n",
    "jan_yellow_sdf_2020_wtemp_and_time = jan_yellow_sdf_2020_wtemp_and_time.withColumn('fare_amount_per_sec', (col(\"fare_amount\") / col(\"trip_time\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16ed6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "columns = ['fare_amount', 'total_amount', 'trip_time', 'trip_distance']\n",
    "imputer = Imputer(inputCols=columns, outputCols=[\"{}_imputed\".format(c) for c in yellow_data_2019_wtemp_and_time.columns]).setStrategy(\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3a507367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# perform one hot encoding for PULocation ID to make continuous and use for regression model.\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder(inputCol=\"PULocationID\",\n",
    "                        outputCol=\"PULocationIDVEC\")\n",
    "model = encoder.fit(yellow_data_2019_wtemp_and_time)\n",
    "encoded = model.transform(yellow_data_2019_wtemp_and_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9dd6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTest = model.transform(jan_yellow_sdf_2020_wtemp_and_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e9ea39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = encoded.select('trip_distance', 'maxTemp', 'trip_time', 'fare_amount')\n",
    "test_data = encodedTest.select('trip_distance', 'maxTemp', 'trip_time', 'fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57a4ff77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trip_distance', 'maxTemp']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = training_data.columns[0:3]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21551c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(outputCol=\"features\")\n",
    "vecAssembler.setInputCols(columns)\n",
    "\n",
    "output = vecAssembler.transform(training_data)\n",
    "final_df = output.select('features', 'fare_amount')\n",
    "\n",
    "output_test = vecAssembler.transform(test_data)\n",
    "final_test = output_test.select('features', 'fare_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b38b2e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/08/15 22:07:40 WARN Instrumentation: [bdb904f7] regParam is zero, which might cause numerical instability and overfitting.\n",
      "21/08/15 22:07:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "21/08/15 22:07:41 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "21/08/15 22:37:32 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\n",
      "21/08/15 22:37:32 WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>trip_distance</th>\n",
       "      <td>1.867850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>trip_time</th>\n",
       "      <td>0.006048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>maxTemp</th>\n",
       "      <td>0.000024</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Coefficients\n",
       "trip_distance      1.867850\n",
       "trip_time          0.006048\n",
       "maxTemp            0.000024"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# implement model\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "lm = LinearRegression(labelCol='fare_amount')\n",
    "model = lm.fit(final_df)\n",
    "pd.DataFrame({\"Coefficients\": model.coefficients}, index=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d7514fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# evaluate residuals on test data\n",
    "res = model.evaluate(final_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "242d0f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           residuals|\n",
      "+--------------------+\n",
      "|-0.05760607432359244|\n",
      "|-0.00714043155607...|\n",
      "|  0.5611334058348678|\n",
      "| 0.17139044038055573|\n",
      "|0.054512131203940495|\n",
      "| 0.46304136726043765|\n",
      "| -0.6778511430152356|\n",
      "|  0.6956665329775138|\n",
      "| -0.4110537134638399|\n",
      "|-0.10649381223197096|\n",
      "| -1.6138240713914485|\n",
      "| -0.2413829802489449|\n",
      "|-0.10894508973362704|\n",
      "| 0.10545255535870801|\n",
      "|  0.4274580390690055|\n",
      "|-0.12816788284072267|\n",
      "|  1.0962795023441565|\n",
      "| -0.5179477987387706|\n",
      "|   -0.53307882088329|\n",
      "|-0.24057673524162304|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "res.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ff23b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  1.3757183648426903\n",
      "Mean squared error:  507.47506900082357\n",
      "r-squared:  -7.310418451391756\n"
     ]
    }
   ],
   "source": [
    "# evaluation\n",
    "print(\"Mean absolute error: \" , res.meanAbsoluteError)\n",
    "print(\"Mean squared error: \" , res.meanSquaredError)\n",
    "print(\"r-squared: \", res.r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "134941b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error:  1.3757204488335586\n",
      "Mean squared error:  507.47990476363395\n",
      "r-squared:  -7.310497641907588\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute error: \" , res.meanAbsoluteError)\n",
    "print(\"Mean squared error: \" , res.meanSquaredError)\n",
    "print(\"r-squared: \", res.r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
